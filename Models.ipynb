{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n",
    "\n",
    "### For missing categorical values imputation have been done by creating a new category \"none\".For numeric values missing values have been imputed by median imputation.\n",
    "### In next steps this imputation would be done by algorithms such as knn and svm and be noted if that leads to improvement in accuarcy of the model.\n",
    "\n",
    "## Data Transformation: \n",
    "\n",
    "### Numeric variables were scaled by minmax scaler as most  of these have different scales.for example budget and num_votes\n",
    "\n",
    "## Scoring\n",
    "\n",
    "### The metric for accuracy has been noted as mean squared error which is one of the most widely used metric for evaluating regression models\n",
    "\n",
    " $$ mse = (\\frac{1}{n})\\sum_{i=1}^{n}(y_{i} - x_{i})^{2} $$\n",
    " \n",
    " ### Lower the MSE better is the model.\n",
    " \n",
    " ## Modelling Techniques\n",
    " \n",
    "### A group of models were evaluated for baselines using sklearn pipeline.The best performing ones were ensemble models hence we evaluated them further by  hyper parameter optimization.The model select was xgboost and it is recommended that the final model of xgboost be deployed as its final parameters reduced MSE by a factor of about 1/6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path=\"movie_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"https://raw.githubusercontent.com/sundeepblue/movie_rating_prediction/master/movie_metadata.csv\"\n",
    "from preprocessing import data_clean\n",
    "\n",
    "data1=data_clean(path)\n",
    "labels=data1.imdb_score.values\n",
    "genres_split=data1.genres.str.get_dummies().add_prefix('Part_')\n",
    "genres_split=genres_split.astype(\"bool\")\n",
    "all_data=pd.concat([data1.drop(columns=[\"genres\"]),genres_split],axis=1)\n",
    "all_data=all_data.drop(columns=[\"imdb_score\"])\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        \n",
    "        ('cat', categorical_transformer, categorical_features)],remainder=\"passthrough\")\n",
    "data_pipeline=Pipeline(steps=[('preprocessor_tree', preprocessor_tree)])\n",
    "data_pipeline.fit(all_data)\n",
    "data_trans=data_pipeline.fit_transform(all_data)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data_trans, labels, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble  import  AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.349\n"
     ]
    }
   ],
   "source": [
    "numeric_features =  all_data.select_dtypes(include=[\"float\"]).columns\n",
    "categorical_features=all_data.select_dtypes(exclude=[\"float\",\"bool\"]).columns\n",
    "bool_features=all_data.select_dtypes(include=[\"bool\"]).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LinearRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data, labels, test_size=0.2,random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model R^2: linear regression  0.28 (+/- 0.07)\n",
      "Model R^2: Lasso -0.00 (+/- 0.00)\n",
      "Model R^2: Ridge 0.28 (+/- 0.07)\n",
      "Model R^2: Decision Trees -0.31 (+/- 0.14)\n",
      "Model R^2: Random Fores 0.36 (+/- 0.05)\n",
      "Model R^2: Ada-Boost 0.14 (+/- 0.05)\n",
      "Model R^2: GBM 0.36 (+/- 0.03)\n",
      "Model R^2: xgboost 0.30 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "regressors = [\n",
    "    (LinearRegression(),\"linear regression \"),\n",
    "    (Lasso(alpha=.5),\"Lasso\"),\n",
    "    (Ridge(alpha=.1),\"Ridge\"),\n",
    "    \n",
    "    (DecisionTreeRegressor(),\"Decision Trees\"),\n",
    "    (RandomForestRegressor(),\"Random Fores\"),\n",
    "    (AdaBoostRegressor(),\"Ada-Boost\"),\n",
    "    (GradientBoostingRegressor(),\"GBM\"),\n",
    "    (XGBRegressor(),\"xgboost\")\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def MSE(y_true,y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print ('MSE: %2.3f' % mse)\n",
    "    return mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def R2(y_true,y_pred):    \n",
    "     r2 = r2_score(y_true, y_pred)\n",
    "     print ('R2: %2.3f' % r2)\n",
    "     return r2\n",
    "\n",
    "def two_score(y_true,y_pred):    \n",
    "    MSE(y_true,y_pred) #set score here and not below if using MSE in GridCV\n",
    "    score = R2(y_true,y_pred)\n",
    "    return score\n",
    "\n",
    "def two_scorer():\n",
    "    return make_scorer(two_score, greater_is_better=True)\n",
    "\n",
    "for r,v in regressors :\n",
    "    \n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', r)])\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores = cross_val_score(clf,X_train, y_train, cv=5,scoring=two_scorer())\n",
    "    \n",
    "    print(\"Model R^2:\"+ \" \" + str(v)+\" \"+\"%0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
